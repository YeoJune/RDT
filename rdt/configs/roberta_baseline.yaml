# RoBERTa Baseline Configuration (Unified Format)

# Model Type
model_type: "mlm" # 'rdt' or 'mlm'

# Model Architecture
model:
  name: "roberta-base" # HuggingFace model name
  # Options: roberta-base, roberta-large, bert-base-uncased, prajjwal1/bert-medium

# Training Strategy
training:
  # Basic settings
  batch_size: 64
  training_mode: "epoch" # 'epoch' or 'step'
  num_epochs: 3
  max_training_steps: 100000 # Used when training_mode is 'step'

  # Optimization
  learning_rate: 0.00005 # 5e-5
  weight_decay: 0.01
  warmup_ratio: 0.06 # ~500 steps for 8000 total steps

  # Scheduler
  scheduler: "linear" # 'cosine' or 'linear'

  # Gradient
  max_grad_norm: 1.0

  # Checkpointing
  save_every_n_epochs: 1
  save_every_n_steps: 1000
  keep_last_n_checkpoints: 3

# Data
data:
  dataset_name: "wikitext-103" # 'wikitext-2' or 'wikitext-103'
  streaming: false
  max_seq_length: 128
  mlm_probability: 0.15 # Standard BERT/RoBERTa masking ratio

  # Tokenizer (auto-loaded from model)
  tokenizer_name: "roberta-base" # Should match model name

  # DataLoader
  num_workers: 4
  pin_memory: true

# Logging & Checkpoints
output:
  checkpoint_dir: "checkpoints/roberta_baseline"
  log_dir: "runs/roberta_baseline"
  log_every_n_steps: 100
  eval_every_n_epochs: 1
  eval_every_n_steps: 1000

# Hardware
device: "cuda"
mixed_precision: true # FP16 training
seed: 42
