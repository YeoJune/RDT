# RDT Base Configuration (Document Recommended Settings)

# Model Type
model_type: "rdt" # 'rdt' or 'mlm'

# Model Architecture
model:
  d_model: 512 # Hidden dimension (auto-set from BERT if use_bert_init=true)
  n_heads: 8 # Attention heads (d_model / 64)
  n_encoder_layers: 4 # Main encoder depth (recursive)
  d_ff: 2048 # FFN dimension (4x d_model)
  dropout: 0.1

  # MLP I/O Processors
  input_processor_layers: 1
  output_processor_layers: 1

  # RoPE Configuration
  rope_base: 10000.0 # RoPE frequency base

  # Gate MLP
  gate_hidden_dim: 128
  gate_num_layers: 2
  gate_num_heads: 4
  gate_dropout: 0.1
  threshold: 0.5 # Gate threshold for stopping criterion

  # Memory optimization
  gradient_checkpointing: false # Enable gradient checkpointing to save memory

  # Weight Tying
  weight_tying: false # Tie output projection weights with token embedding (reduces params, improves training)

  # BERT Weight Initialization
  use_bert_init: false # Initialize from pretrained BERT
  bert_model_name: "prajjwal1/bert-medium" # 8 layers, 512-dim (perfect for n_encoder_layers=6)
  # Options:
  #   - "prajjwal1/bert-medium": 8 layers, 512-dim → n_encoder_layers=6
  #   - "bert-base-uncased": 12 layers, 768-dim → n_encoder_layers=6-10
  #   - "bert-large-uncased": 24 layers, 1024-dim → n_encoder_layers=10-22

# Training Strategy
training:
  # Chain & Masking
  min_chain_length: 1 # Minimum length of training segment
  max_chain_length: 2 # L: Maximum length of training segment
  total_steps: 10 # N: total denoising steps (100% → 0%)
  masking_strategy: "shuffle" # 'shuffle' (index-based)
  visible_loss_ratio: 0.0 # Visible 중에서도 loss 계산할 비율 (안정화)

  # Curriculum Learning
  curriculum:
    enabled: false
    start_step: 3 # Initial max start_step (easy tasks with less masking)

  # BERT-style masking (for masked tokens)
  bert_masking:
    enabled: false # BERT 스타일 마스킹 활성화 여부
    mask_prob: 0.8 # [MASK] 토큰으로 대체할 확률
    random_prob: 0.1 # 랜덤 토큰으로 대체할 확률
    keep_prob: 0.1 # 원본 유지 확률 (합: 1.0)

  # Loss weights
  loss_weight_recon: 1.0 # Reconstruction loss weight
  loss_weight_gate: 1.0 # Gate prediction loss weight
  loss_weight_aux: 0.00001
  aux_ratio: 0.125 # Auxiliary loss micro-batch ratio (0.0-1.0)
  aux_temp: 0.5

  # Optimization
  batch_size: 64
  gradient_accumulation_steps: 4 # Effective batch size = batch_size * gradient_accumulation_steps
  training_mode: "epoch" # 'epoch' or 'step'
  num_epochs: 15 # Used when training_mode is 'epoch'
  max_training_steps: 100000 # Used when training_mode is 'step'
  learning_rate: 0.0001 # 1e-4
  weight_decay: 0.01
  warmup_ratio: 0.05 # 5% of total steps

  # Scheduler
  scheduler: "cosine" # 'cosine' or 'linear'

  # Gradient
  max_grad_norm: 1.0

  # Checkpointing
  save_every_n_epochs: 1 # Used when training_mode is 'epoch'
  save_every_n_steps: 10000 # Used when training_mode is 'step'
  keep_last_n_checkpoints: 3

# Data
data:
  dataset_name: "wikitext-2" # 'wikitext-2' or 'wikitext-103'
  streaming: false # true: buffer-based on-the-fly loading, false: download all at once
  max_seq_length: 128 # Maximum sequence length
  mask_token: "[MASK]"
  samples_per_text: 1 # Number of random samples to generate per text

  # Tokenizer
  tokenizer_name: "bert-base-uncased" # Using standard BERT tokenizer

  # DataLoader
  num_workers: 4
  pin_memory: true
  cache_dir: "./data_cache"

# Logging & Checkpoints
output:
  checkpoint_dir: "checkpoints"
  log_dir: "runs"
  log_every_n_steps: 100
  eval_every_n_epochs: 1 # Used when training_mode is 'epoch'
  eval_every_n_steps: 10000 # Used when training_mode is 'step'
  use_tqdm: true # Enable/disable tqdm progress bars (if false, print logs at log_every_n_steps)

# Hardware
device: "cuda" # 'cuda' or 'cpu'
mixed_precision: false # FP16 training (optional)
seed: 42
