# RDT Base Configuration (Document Recommended Settings)

# Model Architecture
model:
  d_model: 512 # Hidden dimension
  n_heads: 8 # Attention heads (d_model / 64)
  n_encoder_layers: 6 # Shared encoder depth (recursive)
  n_decoder_layers: 1 # Shallow decoder (MAE-style)
  d_ff: 2048 # FFN dimension (4x d_model)
  dropout: 0.1

  # Gate MLP
  gate_hidden_dim: 256 # Gate MLP hidden size

  # Decoder type: 'transformer' or 'linear'
  decoder_type: "transformer" # Extreme case: just projection

# Training Strategy
training:
  # Chain & Masking
  max_chain_length: 10 # L: length of training segment
  total_steps: 10 # N: total denoising steps (100% → 0%)
  masking_strategy: "shuffle" # 'shuffle' (index-based)
  visible_loss_ratio: 0.15 # Visible 중에서도 loss 계산할 비율 (안정화)

  # Loss weights
  loss_weight_recon: 1.0 # Reconstruction loss weight
  loss_weight_gate: 1.0 # Gate prediction loss weight

  # Optimization
  batch_size: 4
  num_epochs: 10
  learning_rate: 0.0001 # 1e-4
  weight_decay: 0.01
  warmup_ratio: 0.1 # 10% of total steps

  # Scheduler
  scheduler: "cosine" # 'cosine' or 'linear'

  # Gradient
  max_grad_norm: 1.0

  # Checkpointing
  save_every_n_epochs: 1
  keep_last_n_checkpoints: 3

# Data
data:
  dataset_name: "wikitext-2" # 'wikitext-2' or 'wikitext-103'
  max_seq_length: 512 # Maximum sequence length
  mask_token: "[MASK]"

  # Tokenizer
  tokenizer_name: "bert-base-uncased" # Using standard BERT tokenizer

  # DataLoader
  num_workers: 4
  pin_memory: true

# Logging & Checkpoints
output:
  checkpoint_dir: "checkpoints"
  log_dir: "runs"
  log_every_n_steps: 100
  eval_every_n_epochs: 1

# Hardware
device: "cuda" # 'cuda' or 'cpu'
mixed_precision: false # FP16 training (optional)
seed: 42
