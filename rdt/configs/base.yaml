# RDT Base Configuration (Document Recommended Settings)

# Model Architecture
model:
  d_model: 512 # Hidden dimension
  n_heads: 8 # Attention heads (d_model / 64)
  n_encoder_layers: 6 # Shared encoder depth (recursive)
  n_io_layers: 1 # Shallow decoder (MAE-style)
  d_ff: 2048 # FFN dimension (4x d_model)
  dropout: 0.1

  # Gate MLP
  gate_scale: 1000
  gate_hidden_dim: 512 # Gate MLP hidden size
  threshold: 0.5 # Gate threshold for stopping criterion

  # Memory optimization
  gradient_checkpointing: false # Enable gradient checkpointing to save memory

# Training Strategy
training:
  # Chain & Masking
  max_chain_length: 5 # L: length of training segment
  total_steps: 20 # N: total denoising steps (100% → 0%)
  masking_strategy: "shuffle" # 'shuffle' (index-based)
  visible_loss_ratio: 0.15 # Visible 중에서도 loss 계산할 비율 (안정화)

  # Loss weights
  loss_weight_recon: 1.0 # Reconstruction loss weight
  loss_weight_gate: 1.0 # Gate prediction loss weight
  loss_weight_aux: 0.1
  aux_ratio: 0.2 # Auxiliary loss micro-batch ratio (0.0-1.0)

  # Optimization
  batch_size: 16
  num_epochs: 10
  learning_rate: 0.0001 # 1e-4
  weight_decay: 0.01
  warmup_ratio: 0.1 # 10% of total steps

  # Scheduler
  scheduler: "cosine" # 'cosine' or 'linear'

  # Gradient
  max_grad_norm: 1.0

  # Checkpointing
  save_every_n_epochs: 1
  keep_last_n_checkpoints: 3

# Data
data:
  dataset_name: "wikitext-2" # 'wikitext-2' or 'wikitext-103'
  max_seq_length: 128 # Maximum sequence length
  mask_token: "[MASK]"
  samples_per_text: 10 # Number of random samples to generate per text

  # Tokenizer
  tokenizer_name: "bert-base-uncased" # Using standard BERT tokenizer

  # DataLoader
  num_workers: 4
  pin_memory: true

# Logging & Checkpoints
output:
  checkpoint_dir: "checkpoints"
  log_dir: "runs"
  log_every_n_steps: 100
  eval_every_n_epochs: 1

# Hardware
device: "cuda" # 'cuda' or 'cpu'
mixed_precision: false # FP16 training (optional)
seed: 42
