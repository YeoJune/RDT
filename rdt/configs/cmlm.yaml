# CMLM (Conditional Masked Language Model) Configuration with RoPE
# Mask-Predict: Parallel decoding with iterative refinement

# Model Type
model_type: "cmlm"

# Model Architecture
model:
  # Architecture dimensions (defaults to BERT-base, override for custom size)
  d_model: 512 # Hidden dimension (BERT-base: 768)
  n_layers: 6 # Number of transformer layers (BERT-base: 12)
  n_heads: 8 # Number of attention heads (BERT-base: 12)
  d_ff: 2048 # Feed-forward dimension (default: 4 * d_model)

  # Sequence and regularization
  max_seq_len: 128 # Maximum sequence length
  dropout: 0.1 # Dropout probability

  # Special tokens
  mask_token_id: 103 # [MASK] token ID (BERT default: 103)
  pad_token_id: 0 # [PAD] token ID (BERT default: 0)

  # RoPE configuration
  rope_base: 10000.0 # RoPE frequency base

  # Weight tying
  tie_weights: true # Tie input/output embeddings (recommended)

# Training Strategy
training:
  # Batch and optimization
  batch_size: 128
  gradient_accumulation_steps: 1 # Effective batch = batch_size * gradient_accumulation_steps
  training_mode: "epoch" # "epoch" or "steps"
  num_epochs: 50
  max_training_steps: 100000

  # Learning rate and scheduling
  learning_rate: 0.0002 # 2e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  scheduler: "cosine"
  max_grad_norm: 1.0

  # Checkpointing
  save_every_n_epochs: 1
  save_every_n_steps: 1000
  keep_last_n_checkpoints: 3

  # BERT-style masking (80% [MASK], 10% random, 10% keep)
  # Set enabled: false to use 100% [MASK] as in CMLM paper
  bert_masking:
    enabled: false

# CMLM-specific Training
cmlm:
  # Uniform masking: masking ratio sampled uniformly from [0, 1]
  # Trains model on both easy (single mask) and difficult (fully masked) examples
  uniform_masking: true

  # Mask-Predict inference settings
  max_iterations: 10 # Number of mask-predict refinement iterations
  mask_decay: "linear" # How mask ratio decreases: 'linear' (1.0 â†’ 0.0)

# Data
data:
  dataset_name: "wikitext-2"
  streaming: false
  max_seq_length: 128

  # CMLM uses uniform masking during training
  # No fixed masking ratio - dynamically sampled per batch

  tokenizer_name: "bert-base-uncased"
  num_workers: 4
  pin_memory: true

# Logging & Checkpoints
output:
  checkpoint_dir: "checkpoints/cmlm"
  log_dir: "runs/cmlm"
  log_every_n_steps: 100
  eval_every_n_epochs: 1
  eval_every_n_steps: 1000
  use_tqdm: true

# Hardware
device: "cuda"
mixed_precision: false
seed: 42
